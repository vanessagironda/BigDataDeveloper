{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef130b84-2f40-4d4a-b193-6289cb4a4523",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pyspark\n",
    "\n",
    "### Agenda \n",
    " \n",
    "1. Introduccion\n",
    "2. Arquitectura\n",
    "3. Tipos de Clusters\n",
    "4. Modulos de Spark\n",
    "5. Spark RDD\n",
    "6. Spark DataFrame\n",
    "7. Spark SQL\n",
    "8. Spark Streaming\n",
    "9. Spark GrpahFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e634f9-8165-4596-bd31-f5e05eebde19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduccion\n",
    "\n",
    "PySpark es una biblioteca de Spark escrita en Python para ejecutar aplicaciones de Python usando las capacidades de Apache Spark, con PySpark podemos ejecutar aplicaciones en paralelo en el clúster distribuido (múltiples nodos).\n",
    "En otras palabras, PySpark es una API de Python para Apache Spark. Apache Spark es un motor de procesamiento analítico para potentes aplicaciones de aprendizaje automático y procesamiento de datos distribuidos a gran escala.\n",
    "\n",
    "##### ¿Quién utiliza Pyspark?\n",
    "PySpark se usa muy bien en la comunidad de ciencia de datos y aprendizaje automático, ya que hay muchas bibliotecas de ciencia de datos ampliamente utilizadas escritas en Python, incluidas NumPy, TensorFlow. También se utiliza debido a su procesamiento eficiente de grandes conjuntos de datos. PySpark ha sido utilizado por muchas organizaciones como Walmart, Trivago, Sanofi, Runtastic y muchas más.\n",
    "\n",
    "##### caracterísicas importantes de Spark\n",
    "![](https://sparkbyexamples.com/wp-content/uploads/2020/08/pyspark-features-1.png)\n",
    "# \n",
    "- Cálculo en memoria\n",
    "- Procesamiento distribuido usando paralelizar\n",
    "- Se puede usar con muchos administradores de clústeres (Spark, Yarn, Mesos, etc.)\n",
    "- Tolerante a fallos\n",
    "- Inmutable\n",
    "- Evaluación perezosa\n",
    "- Caché y persistencia\n",
    "- Optimización incorporada al usar DataFrames\n",
    "- Soporta ANSI SQL\n",
    "\n",
    "##### Ventajas\n",
    "#\n",
    "- Spark es un motor de procesamiento distribuido, en memoria y de uso general que le permite procesar datos de manera eficiente y distribuida.\n",
    "- Obtendrá grandes beneficios al usar Spark para canalizaciones de ingesta de datos.\n",
    "- Con Spark podemos procesar datos de Hadoop HDFS, AWS S3 y muchos sistemas de archivos.\n",
    "- PySpark también se usa para procesar datos en tiempo real usando Streaming y Kafka.\n",
    "- Con spark streaming, también puede transmitir archivos desde el sistema de archivos y también desde el socket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc13b97c-6e22-49ca-b945-efd7522cb938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Arquitectura\n",
    "# \n",
    "Apache Spark funciona en una arquitectura master-slave donde el master se llama \"driver\" y los esclavos se llaman \"Workers\". Cuando ejecuta una aplicación Spark, Spark Driver crea un contexto que es un punto de entrada a su aplicación, y todas las operaciones (transformaciones y acciones) se ejecutan en los workers y los recursos son administrados por el Administrador de clústeres.\n",
    "\n",
    "![](https://sparkbyexamples.com/wp-content/uploads/2020/02/spark-cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c710ab46-0bd0-4e2e-94a8-d573e24ace0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Tipos de clusters\n",
    "#\n",
    "- Standalone –  un administrador de clúster simple incluido con Spark que facilita la configuración de un clúster.\n",
    "- Apache Mesos – Mesos es un administrador de clústeres que también puede ejecutar aplicaciones Hadoop MapReduce y PySpark.\n",
    "- Hadoop YARN –  el administrador de recursos en Hadoop 2. Esto se usa principalmente como administrador de clústeres.\n",
    "- Kubernetes – un sistema de código abierto para automatizar la implementación, el escalado y la gestión de aplicaciones en contenedores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e08b3519-519b-48ff-ac3d-49a903e3d192",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Módulos de Spark\n",
    "\n",
    "![](https://tse1.mm.bing.net/th?id=OIP.IH66oypTYnWQjFkFHoBMCgHaD2&pid=Api&P=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fbe660-9e52-46cd-b766-f32fe0269c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark RDD\n",
    "\n",
    "RDD es una estructura de datos fundamental de Spark que es una colección de objetos distribuidos inmutables y tolerantes a fallas, lo que significa que una vez que crea un RDD no puede cambiarlo. Cada conjunto de datos en RDD se divide en particiones lógicas, que se pueden calcular en diferentes nodos del clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ae8225-89ee-400e-a966-17fbea3f26e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m878.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840644 sha256=d57223fdd14f791b97658fad06ffb6c5a4a48ab009413efbafa3032daf4b077c\n",
      "  Stored in directory: /root/.cache/pip/wheels/2e/d2/18/6f4f20e8332359f7fffceb6828edcc80ef96f86744192a7bb9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2d401e-1301-46b2-8266-032be599e883",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/16 02:12:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"RDD\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2afa44-0561-4034-8330-b0bc9d7ac31b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Java', 20000), ('Python', 100000), ('Scala', 3000)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creacion de un RDD utilizando parallelize\n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd = spark.sparkContext.parallelize(dataList)\n",
    "rdd.collect() #accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b7fcec-8c9f-4626-94ff-ec2271aa8adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creación de un RDD utilizando un archivo\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/test.txt\")\n",
    "rdd2.collect() #accion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e149a9ad-01dc-4439-9d37-6fa33eb064e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### Operaciones RDD\n",
    "En PySpark RDD, puede realizar dos tipos de operaciones.\n",
    "\n",
    "Transformaciones RDD:  las transformaciones son operaciones perezosas. Cuando ejecuta una transformación (por ejemplo, una actualización), en lugar de actualizar un RDD actual, estas operaciones devuelven otro RDD.\n",
    "\n",
    "Acciones de RDD  : operaciones que activan el cálculo y devuelven valores de RDD al controlador.\n",
    "\n",
    "##### Transformaciones RDD\n",
    "Las transformaciones en Spark RDD  devuelven otro RDD y las transformaciones son perezosas, lo que significa que no se ejecutan hasta que llamas a una acción en RDD. Algunas transformaciones en los RDD son flatMap(), map(), reduceByKey(), filter(), sortByKey() y devuelven un nuevo RDD en lugar de actualizar el actual.\n",
    "\n",
    "##### Acciones de RDD\n",
    "La operación Acción de RDD devuelve los valores de un RDD a un nodo de controlador. En otras palabras, cualquier función RDD que no devuelva RDD[T] se considera una acción. \n",
    "\n",
    "Algunas acciones en RDD son count(), collect(), first(), max()y reduce() y más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d20386e9-2c95-4d49-8bc1-8f9d3e6453bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "DataFrame es una colección distribuida de datos organizados en columnas con nombre. Es conceptualmente equivalente a una tabla en una base de datos relacional o un marco de datos en R/Python, pero con optimizaciones más ricas bajo el capó. Los marcos de datos se pueden construir a partir de una amplia gama de fuentes, como archivos de datos estructurados, tablas en Hive, bases de datos externas o RDD existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8403872-1c81-46f8-a42c-d9c266d0a785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creación de un dataframe utilizando createDataFrame\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a6945d0-6198-4ffc-bc14-a9bd7f1bb7a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display(df) \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7777187f-9a83-4e9c-bbcd-5d37015180a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creación de un dataframe utilizando una fuente externa\n",
    "df = spark.read.csv(\"/tmp/resources/zipcodes.csv\") # spark.read.json/ .parquet ( spark.read.format().load())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6639e965-86d4-4f59-8115-5588db9c5c37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark SQL\n",
    "\n",
    "Spark SQL en Apache Spark es uno de los módulos para el procesamiento de la información que ofrece Apache Spark y que trabaja con datos estructurados.\n",
    "Una vez que haya creado un DataFrame, puede interactuar con los datos utilizando la sintaxis SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0619ae59-15c7-44ec-b3e7-71c2d89cbfb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"PERSON_DATA\")\n",
    "df.cache()\n",
    "df.count()\n",
    "df2 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0145ee84-c4b0-4da5-a1f4-0a9304cd5718",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark Streaming\n",
    "Spark Streaming es un sistema de procesamiento de transmisión escalable, de alto rendimiento y tolerante a fallas que admite cargas de trabajo tanto por lotes como de transmisión. Se utiliza para procesar datos en tiempo real de fuentes como la carpeta del sistema de archivos, el socket TCP, S3 , Kafka , Flume , Twitter y Amazon Kinesis , por nombrar algunos. Los datos procesados se pueden enviar a bases de datos, Kafka, dashboard, etc.\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/streaming-arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12f7e519-1b2a-4254-af0e-fedc5c66962a",
     "showTitle": true,
     "title": "Streaming con socket"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\n",
    "      .format(\"socket\")\n",
    "      .option(\"host\",\"localhost\")\n",
    "      .option(\"port\",\"9090\")\n",
    "      .load()\n",
    "        \n",
    "query = df.writeStream\n",
    "      .format(\"console\")\n",
    "      .outputMode(\"updated\")\n",
    "      .start()\n",
    "      .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7725bf95-6aba-4ed7-be82-42d4aacd1ed2",
     "showTitle": true,
     "title": "Streaming con kafka"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "        .option(\"subscribe\", \"json_topic\")\n",
    "        .option(\"startingOffsets\", \"earliest\") // From starting\n",
    "        .load()\n",
    "        \n",
    "df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "   .writeStream\n",
    "   .format(\"kafka\")\n",
    "   .outputMode(\"append\")\n",
    "   .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\n",
    "   .option(\"topic\", \"josn_data_topic\")\n",
    "   .start()\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79195a12-e27e-4dbf-b8db-0ab2a2787cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark GrpahFrame\n",
    "\n",
    "GraphFrames es un paquete para Apache Spark que proporciona gráficos basados ​​en DataFrame. Proporciona API de alto nivel en Scala, Java y Python. Su objetivo es proporcionar tanto la funcionalidad de GraphX como la funcionalidad extendida aprovechando Spark DataFrames. Esta funcionalidad ampliada incluye búsqueda de motivos, serialización basada en DataFrame y consultas gráficas altamente expresivas."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Introduccion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
